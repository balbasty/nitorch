import math
import torch
import torch.distributions as td
from nitorch.core.utils import unsqueeze, ensure_shape
from nitorch.core.constants import pi
from nitorch.core.kernels import smooth
from nitorch.core import utils, py, linalg
from nitorch import spatial
from nitorch.nn.base import Module
from .utils import batch_callable, padshape, call_spatial


class RandomField(Module):
    """Sample a smooth random field.

    The field is generated by sampling white Gaussian noise and
    convolving it with a Gaussian kernel.

    """

    def __init__(self, shape=None, mean=0, amplitude=1, fwhm=1, channel=1,
                 basis=1, device='cpu', dtype=None):
        """

        Parameters
        ----------
        shape : sequence[int], optional
            Lattice shape
        mean : callable or tensor, default=0
            Mean value. Should be of shape (channel,) or (channel, *shape)
        amplitude : callable or tensor, default=1
            Amplitude of the squared-exponential kernel.
            Should broadcast to (channel, *shape)
        fwhm : callable or tensor, default=1
            Full-width at Half Maximum of the squared-exponential kernel.
            Should broadcast to (channel, ndim)
        channel : int, default=1
            Number of channels
        basis : {0, 1}, default=1
            See `nitorch.core.kernels.smooth`
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.

        """
        super().__init__()
        self.shape = shape
        self.mean = mean
        self.amplitude = amplitude
        self.fwhm = fwhm
        self.channel = channel
        self.basis = basis
        self.device = device
        if dtype is None or not dtype.is_floating_point:
            dtype = torch.get_default_dtype()
        self.dtype = dtype

    def to(self, *args, **kwargs):
        device, dtype, non_blocking, convert_to_format \
            = torch._C._nn._parse_to(*args, **kwargs)

        self.dtype = dtype or self.dtype
        self.device = device or self.device
        super().to(*args, **kwargs)

    def forward(self, batch=1, **overload):
        """

        Parameters
        ----------
        batch : int, default=1
            Batch size
        overload : dict

        Returns
        -------
        field : (batch, channel, *shape) tensor
            Generated random field

        """

        # get arguments
        shape = overload.get('shape', self.shape)
        mean = overload.get('mean', self.mean)
        amplitude = overload.get('amplitude', self.amplitude)
        fwhm = overload.get('fwhm', self.fwhm)
        channel = overload.get('channel', self.channel)
        basis = overload.get('basis', self.basis)
        dtype = overload.get('dtype', self.dtype)
        device = overload.get('device', self.device)
        backend = dict(dtype=dtype, device=device)

        # sample if parameters are callable
        nb_dim = len(shape)
        mean = call_spatial(mean, batch, nb_dim)           # ([B], C, *spatial)
        amplitude = call_spatial(amplitude, batch, nb_dim) # ([B], C, *spatial)
        fwhm = call_spatial(fwhm, batch, 1)                # ([B], D)

        # device/dtype
        mean = torch.as_tensor(mean, **backend)
        amplitude = torch.as_tensor(amplitude, **backend)
        fwhm = torch.as_tensor(fwhm, **backend)

        # reshape
        full_shape = [batch, channel, *shape]
        mean = mean.expand(full_shape)
        amplitude = amplitude.expand(full_shape)
        fwhm = fwhm.expand([batch, channel, nb_dim])

        conv = torch.nn.functional.conv1d if nb_dim == 1 else \
               torch.nn.functional.conv2d if nb_dim == 2 else \
               torch.nn.functional.conv3d if nb_dim == 3 else None
        
        # convert SE parameters to noise/kernel parameters
        sigma_se = fwhm / math.sqrt(8*math.log(2))
        sigma_se = unsqueeze(sigma_se.prod(dim=-1), dim=-1, ndim=nb_dim)
        amplitude = amplitude * (2*pi)**(nb_dim/4) * sigma_se.sqrt()
        fwhm = fwhm * math.sqrt(2)
        
        # smooth
        samples_b = []
        for b in range(batch):
            samples_c = []
            for c in range(channel):
                kernel = smooth('gauss', fwhm[b, c], basis=basis,
                                device=device, dtype=dtype)

                # compute input shape
                pad_shape = [shape[d] + kernel[d].shape[d+2] - 1
                             for d in range(nb_dim)]
                mean1 = ensure_shape(mean[b, c], pad_shape,
                                     mode='reflect2', side='both')
                amplitude1 = ensure_shape(amplitude[b, c], pad_shape,
                                          mode='reflect2', side='both')

                # generate sample
                sample = torch.distributions.Normal(mean1, amplitude1).sample()
                sample = sample[None, None, ...]

                # convolve
                for ker in kernel:
                    sample = conv(sample, ker)

                samples_c.append(sample)

            samples_b.append(torch.cat(samples_c, dim=1))

        sample = torch.cat(samples_b, dim=0)

        return sample


class RandomFieldSpline(Module):
    """Sample a smooth random field.

    The field is generated by sampling b-spline coefficients
    from a Gaussian distribution.

    """

    def __init__(self, shape=None, mean=0, amplitude=1, fwhm=1, channel=1,
                 basis=3, device='cpu', dtype=None):
        """

        Parameters
        ----------
        shape : sequence[int], optional
            Lattice shape
        mean : callable or tensor, default=0
            Mean value. Should broadcast to (channel, *shape)
        amplitude : callable or tensor, default=1
            Amplitude of the squared-exponential kernel.
            Should broadcast to (channel,)
        fwhm : callable or tensor, default=1
            Full-width at Half Maximum of the squared-exponential kernel.
            Should broadcast to (ndim,)
        channel : int, default=1
            Number of channels
        basis : {0, 1}, default=1
            See `nitorch.core.kernels.smooth`
        order : {0..7}, default=None
            Use b-spline of this order instead of Gaussian smothing.
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.

        """
        super().__init__()
        self.shape = shape
        self.mean = mean
        self.amplitude = amplitude
        self.fwhm = fwhm
        self.channel = channel
        self.basis = basis
        self.device = device
        if dtype is None or not dtype.is_floating_point:
            dtype = torch.get_default_dtype()
        self.dtype = dtype

    def forward(self, batch=1, **overload):
        """

        Parameters
        ----------
        batch : int, default=1
            Batch size
        overload : dict

        Returns
        -------
        field : (batch, channel, *shape) tensor
            Generated random field

        """

        # get arguments
        shape = overload.get('shape', self.shape)
        mean = overload.get('mean', self.mean)
        amplitude = overload.get('amplitude', self.amplitude)
        fwhm = overload.get('fwhm', self.fwhm)
        channel = overload.get('channel', self.channel)
        basis = overload.get('basis', self.basis)
        dtype = overload.get('dtype', self.dtype)
        device = overload.get('device', self.device)
        backend = dict(dtype=dtype, device=device)

        # sample if parameters are callable
        nb_dim = len(shape)
        mean = call_spatial(mean, batch, nb_dim)           # ([B], C, *spatial)
        amplitude = call_spatial(amplitude, batch, nb_dim) # ([B], C, *spatial)
        fwhm = call_spatial(fwhm, batch, 1)                # ([B], D)

        # device/dtype
        mean = torch.as_tensor(mean, **backend)
        amplitude = torch.as_tensor(amplitude, **backend)
        fwhm = torch.as_tensor(fwhm, **backend)

        # reshape
        nb_dim = len(shape)
        fwhm = utils.make_vector(fwhm, nb_dim)

        # sample spline coefficients
        nodes = [(s/f).ceil().int().item() for s, f in zip(shape, fwhm)]
        sample = torch.randn([batch, channel, *nodes], **backend)
        sample *= amplitude
        sample = spatial.resize(sample, shape=shape, interpolation=basis,
                                bound='dct2')
        sample += mean

        return sample


class RandomFieldGreens(Module):
    """Sample a Gaussian random field defined by its Greens function."""

    def __init__(self, shape=None, mean=0, channel=1,
                 absolute=1e-3, membrane=0.1, bending=0, voxel_size=1,
                 cache_greens=True, device='cpu', dtype=None):
        """

        Parameters
        ----------
        shape : sequence[int], optional
            Lattice shape
        mean : callable or tensor, default=0
            Mean value. Should broadcast to (channel, *shape)
        channel : int, default=1
            Number of channels
        absolute : callable or tensor, default=1e-4
            Penalty on absolute displacements.
            Should broadcast to (channel,)
        membrane : callable or tensor, default=0.1
            Penalty on membrane energy (first derivatives).
            Should broadcast to (channel,)
        bending : callable or tensor, default=0
            Penalty on bending energy (second derivatives).
            Should broadcast to (channel,)
        voxel_size : float or sequence[float], default=1
            Voxel size of the lattice.
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.

        """
        super().__init__()
        self.shape = shape
        self.mean = mean
        self.channel = channel
        self.absolute = absolute
        self.membrane = membrane
        self.bending = bending
        self.voxel_size = voxel_size
        self.cache_greens = cache_greens
        self.device = device
        if dtype is None or not dtype.is_floating_point:
            dtype = torch.get_default_dtype()
        self.dtype = dtype

    def forward(self, batch=1, **overload):
        """

        Parameters
        ----------
        batch : int, default=1
            Batch size
        overload : dict

        Returns
        -------
        field : (batch, channel, *shape) tensor
            Generated random field

        """

        # get arguments
        shape = overload.get('shape', self.shape)
        mean = overload.get('mean', self.mean)
        channel = overload.get('channel', self.channel)
        absolute = overload.get('absolute', self.absolute)
        membrane = overload.get('membrane', self.membrane)
        bending = overload.get('bending', self.bending)
        voxel_size = overload.get('voxel_size', self.voxel_size)
        dtype = overload.get('dtype', self.dtype)
        device = overload.get('device', self.device)
        backend = dict(dtype=dtype, device=device)

        # sample if parameters are callable
        mean = mean() if callable(mean) else mean
        absolute = absolute() if callable(absolute) else absolute
        membrane = membrane() if callable(membrane) else membrane
        bending = bending() if callable(bending) else bending

        # reshape
        nb_dim = len(shape)
        full_shape = [batch, channel, *shape]
        mean = torch.as_tensor(mean, **backend).expand(full_shape)
        absolute = utils.make_vector(absolute, channel, **backend)
        membrane = utils.make_vector(membrane, channel, **backend)
        bending = utils.make_vector(bending, channel, **backend)
        if torch.is_tensor(voxel_size):
            voxel_size = voxel_size.tolist()
        voxel_size = py.make_list(voxel_size, nb_dim)

        # sample white noise
        sample = torch.randn([batch, channel, *shape, 2], **backend)

        # create greens kernel
        for c in range(channel):
            if (hasattr(self, '_greens') and
                    absolute[c] == self._absolute and
                    membrane[c] == self._membrane and
                    bending[c] == self._bending and
                    voxel_size == self._voxel_size):
                greens = self._greens.to(dtype=dtype, device=device)
            else:
                greens = spatial.greens(shape,
                                        absolute=absolute[c],
                                        membrane=membrane[c],
                                        bending=bending[c],
                                        lame=0,
                                        voxel_size=voxel_size,
                                        device=device,
                                        dtype=dtype)
                greens = greens.sqrt()
                if self.cache_greens:
                    self._greens = greens
                    self._absolute = absolute[c]
                    self._membrane = membrane[c]
                    self._bending = bending[c]
                    self._voxel_size = voxel_size

            # multiply by square root of greens
            sample[:, c] = sample[:, c] * greens[..., None]

        # inverse Fourier transform
        if utils.torch_version('>=', (1, 8)):
            sample = torch.fft.ifftn(sample, dim=nb_dim).real()
        else:
            sample = torch.ifft(sample, nb_dim)[..., 0]
        sample *= py.prod(shape)

        # add mean
        sample += mean

        return sample


class RandomGridGreens(Module):
    """Sample a Gaussian random field defined by its Greens function."""

    def __init__(self, shape=None, mean=0,
                 absolute=1e-4, membrane=1e-3, bending=0.2, lame=(0.05, 0.2),
                 voxel_size=1, cache_greens=True, device='cpu', dtype=None):
        """

        Parameters
        ----------
        shape : sequence[int], optional
            Lattice shape
        mean : callable or tensor, default=0
            Mean value. Should broadcast to (channel, *shape)
        absolute : callable or tensor, default=1e-4
            Penalty on absolute displacements.
            Should broadcast to (channel,)
        membrane : callable or tensor, default=1e-3
            Penalty on membrane energy (first derivatives).
            Should broadcast to (channel,)
        bending : callable or tensor, default=0.2
            Penalty on bending energy (second derivatives).
            Should broadcast to (channel,)
        lame : pair of [callable or tensor], default=(0.05, 0.2)
            Penalty on linear-elastic energy (zooms and shears).
            Should broadcast to (channel,)
        voxel_size : float or sequence[float], default=1
            Voxel size of the lattice.
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.

        """
        super().__init__()
        self.shape = shape
        self.mean = mean
        self.absolute = absolute
        self.membrane = membrane
        self.bending = bending
        self.lame = lame
        self.voxel_size = voxel_size
        self.cache_greens = cache_greens
        self.device = device
        if dtype is None or not dtype.is_floating_point:
            dtype = torch.get_default_dtype()
        self.dtype = dtype

    def forward(self, batch=1, **overload):
        """

        Parameters
        ----------
        batch : int, default=1
            Batch size
        overload : dict

        Returns
        -------
        field : (batch, channel, *shape) tensor
            Generated random field

        """

        # get arguments
        shape = overload.get('shape', self.shape)
        mean = overload.get('mean', self.mean)
        absolute = overload.get('absolute', self.absolute)
        membrane = overload.get('membrane', self.membrane)
        bending = overload.get('bending', self.bending)
        lame = overload.get('lame', self.lame)
        voxel_size = overload.get('voxel_size', self.voxel_size)
        dtype = overload.get('dtype', self.dtype)
        device = overload.get('device', self.device)
        backend = dict(dtype=dtype, device=device)

        # sample if parameters are callable
        mean = mean() if callable(mean) else mean
        absolute = absolute() if callable(absolute) else absolute
        membrane = membrane() if callable(membrane) else membrane
        bending = bending() if callable(bending) else bending
        lame1, lame2 = py.make_list(lame, 2)
        lame1 = lame1() if callable(lame1) else lame1
        lame2 = lame2() if callable(lame2) else lame2

        # reshape
        nb_dim = len(shape)
        full_shape = [batch, *shape, nb_dim]
        mean = torch.as_tensor(mean, **backend).expand(full_shape)
        absolute = torch.as_tensor(absolute, **backend).flatten()[0]
        membrane = torch.as_tensor(membrane, **backend).flatten()[0]
        bending = torch.as_tensor(bending, **backend).flatten()[0]
        lame1 = torch.as_tensor(lame1, **backend).flatten()[0]
        lame2 = torch.as_tensor(lame2, **backend).flatten()[0]
        if torch.is_tensor(voxel_size):
            voxel_size = voxel_size.tolist()
        voxel_size = utils.make_vector(voxel_size, nb_dim, **backend)

        # sample white noise
        sample = torch.randn([2, batch, *shape, nb_dim], **backend)

        # create greens kernel
        if (hasattr(self, '_greens') and
                absolute == self._absolute and
                membrane == self._membrane and
                bending == self._bending and
                lame1 == self._lame1 and
                lame2 == self._lame2 and
                voxel_size == self._voxel_size):
            greens = self._greens.to(dtype=dtype, device=device)
        else:
            greens = spatial.greens(shape,
                                    absolute=absolute,
                                    membrane=membrane,
                                    bending=bending,
                                    lame=(lame1, lame2),
                                    voxel_size=voxel_size,
                                    device=device,
                                    dtype=dtype)
            if lame1 or lame2:
                greens = greens.cholesky(upper=False)
            else:
                greens = greens.sqrt()
            if self.cache_greens:
                self._greens = greens
                self._absolute = absolute
                self._membrane = membrane
                self._bending = bending
                self._lame1 = lame1
                self._lame2 = lame2
                self._voxel_size = voxel_size

        # multiply by square root of greens
        if lame1 or lame2:
            sample = linalg.matvec(greens, sample)
        else:
            sample = sample * greens[..., None]
            sample = sample / voxel_size.sqrt()

        # inverse Fourier transform
        sample = utils.movedim(sample, 0, -1)
        sample = utils.movedim(sample, -2, 1)
        if utils.torch_version('>=', (1, 8)):
            sample = torch.fft.ifftn(sample, dim=nb_dim).real()
        else:
            sample = torch.ifft(sample, nb_dim)[..., 0]
        sample = utils.movedim(sample, 1, -1)
        sample *= py.prod(shape)

        # add mean
        sample += mean

        return sample


class RandomMultiplicativeField(Module):

    def __init__(self, mean=None, amplitude=None, fwhm=None,
                 device='cpu', dtype=None):
        """

        Parameters
        ----------
        mean : callable or tensor, default=0
            Log-Mean value. Should broadcast to (channel, *shape)
        amplitude : callable or tensor, default=LogNormal(log(1), log(10)/3)
            Amplitude of the squared-exponential kernel.
            Should broadcast to (channel, *shape)
        fwhm : callable or tensor, default=LogNormal(log(5), log(2)/3)
            Full-width at Half Maximum of the squared-exponential kernel.
            Should broadcast to (channel, ndim)
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.
        """

        super().__init__()
        mean = mean if mean is not None else self.default_mean
        amplitude = amplitude if amplitude is not None else self.default_amplitude
        fwhm = fwhm if fwhm is not None else self.default_fwhm
        self.field = RandomFieldSpline(mean=mean, amplitude=amplitude,
                                       fwhm=fwhm, device=device, dtype=dtype)

    # defer properties
    mean = property(lambda self: self.field.mean)
    amplitude = property(lambda self: self.field.amplitude)
    fwhm = property(lambda self: self.field.fwhm)

    # default generators
    default_mean = staticmethod(lambda *b: torch.zeros(b))

    @staticmethod
    def default_amplitude(*b):
        return td.Normal(0., math.log(10)/3).sample(*b).exp()

    @staticmethod
    def default_fwhm(*b):
        return td.Normal(math.log(32.), math.log(3)/3).sample(*b).exp()

    def to(self, *args, **kwargs):
        self.field.to(*args, **kwargs)
        super().to(*args, **kwargs)

    def forward(self, shape, **overload):
        """

        Parameters
        ----------
        shape : sequence[int]
            (batch, channel, *spatial)
        overload : dict

        Returns
        -------
        bias : (batch, channel, *shape)

        """

        bias = self.field(batch=shape[0], channel=shape[1],
                          shape=shape[2:], **overload)
        return bias.exp()


class BiasFieldTransform(Module):
    """Apply a random multiplicative bias field to an image."""

    def __init__(self, mean=None, amplitude=None, fwhm=None,
                 device='cpu', dtype=None):
        """

        Parameters
        ----------
        mean : callable or tensor, default=0
            Log-Mean value. Should broadcast to (channel, *shape)
        amplitude : callable or tensor, default=LogNormal(log(1), log(10)/3)
            Amplitude of the squared-exponential kernel.
            Should broadcast to (channel, *shape)
        fwhm : callable or tensor, default=LogNormal(log(5), log(2)/3)
            Full-width at Half Maximum of the squared-exponential kernel.
            Should broadcast to (channel, ndim)
        device : torch.device: default='cpu'
            Output tensor device.
        dtype : torch.dtype, default=torch.get_default_dtype()
            Output tensor datatype.
        """

        super().__init__()
        mean = mean if mean is not None else self.default_mean
        amplitude = amplitude if amplitude is not None else self.default_amplitude
        fwhm = fwhm if fwhm is not None else self.default_fwhm
        self.field = RandomFieldSpline(mean=mean, amplitude=amplitude,
                                       fwhm=fwhm, device=device, dtype=dtype)

    # defer properties
    mean = property(lambda self: self.field.mean)
    amplitude = property(lambda self: self.field.amplitude)
    fwhm = property(lambda self: self.field.fwhm)

    # default generators
    default_mean = staticmethod(lambda *b: torch.zeros(b))

    @staticmethod
    def default_amplitude(*b):
        return td.Normal(0., math.log(10)/3).sample(b).exp()

    @staticmethod
    def default_fwhm(*b):
        return td.Normal(math.log(32.), math.log(3)/3).sample(b).exp()

    def to(self, *args, **kwargs):
        self.field.to(*args, **kwargs)
        super().to(*args, **kwargs)

    def forward(self, image, **overload):
        """

        Parameters
        ----------
        image : (batch, channel, *shape)
            Input tensor
        overload : dict

        Returns
        -------
        transformed_image : (batch, channel, *shape)
            Bias-multiplied tensor

        """

        image = torch.as_tensor(image)
        overload['dtype'] = image.dtype
        overload['device'] = image.device
        bias = self.field(batch=image.shape[0], channel=image.shape[1],
                          shape=image.shape[2:], **overload)
        image = image * bias.exp()
        return image
