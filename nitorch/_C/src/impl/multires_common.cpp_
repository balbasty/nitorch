/*
I am starting to work on a more stable (and faster) prolong/restrict.
To make it simpler, I'll specialize it for linear restriction // quadratic prolongation
since that's all we need so far. We can always use the more versatile "resize"
for other cases.
I think I'll start with the assumption that the scale is approx 2, but we could 
have a generic implementation for any scale (with a for loop) and a fast 
implementation for scale == 2.
The main difference with the implementation in "resize" is that (in restrict) 
we don't rely on atromic adds, but instead manually sum voxels 
that we know should be pushed in a given output voxel. We therefore loop 
about the output voxels (instead of the "target" voxels in resize).

This is WIP and currently does not even compile.
*/

#include "common.h"                // write C++/CUDA compatible code
#include "../defines.h"            // useful macros
#include "bounds_common.h"         // boundary conditions + enum
#include "interpolation_common.h"  // interpolation weights + enum
#include "allocator.h"             // base class handling offset sizes
#include <ATen/ATen.h>             // tensors
#include <tuple>                   // needed by prepare_tensors

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// CPU/GPU -specific parameters
#ifdef __CUDACC__
# include <ATen/cuda/CUDAContext.h>
# include <ATen/cuda/detail/KernelUtils.h>
# include <c10/macros/Macros.h>
  using namespace at::cuda::detail;
#else
# include <ATen/Parallel.h>
  namespace {
    // This parameter specifies the minimum number of voxels that should be 
    // processed on a single processor in the parallel for loop .
    int64_t GRAIN_SIZE = static_cast<int64_t>(at::internal::GRAIN_SIZE);
  }
#endif
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// maximum number of channels
// > not used in mode isotropic nearest/linear
// We override the (small) default
#undef  NI_MAX_NUM_CHANNELS
#define NI_MAX_NUM_CHANNELS 1024

#define VEC_UNFOLD(ONAME, INAME, DEFAULT)             \
  ONAME##0(INAME.size() > 0 ? INAME[0] : DEFAULT),  \
  ONAME##1(INAME.size() > 1 ? INAME[1] :            \
           INAME.size() > 0 ? INAME[0] : DEFAULT),  \
  ONAME##2(INAME.size() > 2 ? INAME[2] :            \
           INAME.size() > 1 ? INAME[1] :            \
           INAME.size() > 0 ? INAME[0] : DEFAULT)

using at::Tensor;
using at::TensorOptions;
using c10::IntArrayRef;
using c10::ArrayRef;

namespace ni {
NI_NAMESPACE_DEVICE { // cpu / cuda / ...

namespace { // anonymous namespace > everything inside has internal linkage

using L = InterpolationType::Linear;
using Q = InterpolationType::Quadratic;

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                        INDEXING UTILS
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

class MultiResAllocator: public Allocator {
public:

  // ~~~ CONSTRUCTORS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  NI_HOST
  ResizeAllocator(int dim, BoundVectorRef bound,
                    ArrayRef<double> scale,
                    bool do_adjoint):
    dim(dim),
    VEC_UNFOLD(bound,         bound,         BoundType::Replicate),
    VEC_UNFOLD(scale,         scale,         1.),
    do_adjoint(do_adjoint)
  {}

  // ~~~ FUNCTORS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


  NI_HOST void ioset
  (const Tensor& input, const Tensor& output)
  {
    init_all();
    init_input(input);
    init_output(output);
  }

  // We just check that all tensors that we own are compatible with 32b math
  bool canUse32BitIndexMath(int64_t max_elem=max_int32) const
  {
    return inp_32b_ok && out_32b_ok;
  }

private:

  // ~~~ COMPONENTS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  NI_HOST void init_all();
  NI_HOST void init_input(const Tensor& input);
  NI_HOST void init_output(const Tensor& output);

  // ~~~ OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  int               dim;            // dimensionality (2 or 3)
  BoundType         bound0;         // boundary condition  // x|W
  BoundType         bound1;         // boundary condition  // y|H
  BoundType         bound2;         // boundary condition  // z|D
  double            scale0;         // scale               // x|W
  double            scale1;         // scale               // y|H
  double            scale2;         // scale               // z|D
  bool              do_adjoint;     // push instead of pull

  // ~~~ NAVIGATORS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#define DECLARE_ALLOC_INFO_5D(NAME)  \
  int64_t NAME##_X;                 \
  int64_t NAME##_Y;                 \
  int64_t NAME##_Z;                 \
  int64_t NAME##_sN;                \
  int64_t NAME##_sC;                \
  int64_t NAME##_sX;                \
  int64_t NAME##_sY;                \
  int64_t NAME##_sZ;                \
  bool NAME##_32b_ok;               \
  void * NAME##_ptr;

  int64_t N;
  int64_t C;
  DECLARE_ALLOC_INFO_5D(inp)
  DECLARE_ALLOC_INFO_5D(out)

  // Allow ResizeImpl's constructor to access ResizeAllocator's
  // private members.
  template <typename scalar_t, typename offset_t>
  friend class MultiResImpl;
};

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                          INITIALISATION
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


NI_HOST
void ResizeAllocator::init_all()
{
  N = C = 1L;
  inp_X = inp_Y = inp_Z = 1L;
  out_X = out_Y = out_Z = 1L;
  inp_ptr = out_ptr = static_cast<float*>(0);
  inp_32b_ok = out_32b_ok = true;
}

NI_HOST
void ResizeAllocator::init_input(const Tensor& input)
{
  N       = input.size(0);
  C       = input.size(1);
  inp_X   = input.size(2);
  inp_Y   = dim < 2 ? 1L : input.size(3);
  inp_Z   = dim < 3 ? 1L : input.size(4);
  inp_sN  = input.stride(0);
  inp_sC  = input.stride(1);
  inp_sX  = input.stride(2);
  inp_sY  = dim < 2 ? 0L : input.stride(3);
  inp_sZ  = dim < 3 ? 0L : input.stride(4);
  inp_ptr = input.data_ptr();
  inp_32b_ok = tensorCanUse32BitIndexMath(input);
}

NI_HOST
void ResizeAllocator::init_output(const Tensor& input)
{
  out_X   = input.size(2);
  out_Y   = dim < 2 ? 1L : input.size(3);
  out_Z   = dim < 3 ? 1L : input.size(4);
  out_sN  = input.stride(0);
  out_sC  = input.stride(1);
  out_sX  = input.stride(2);
  out_sY  = dim < 2 ? 0L : input.stride(3);
  out_sZ  = dim < 3 ? 0L : input.stride(4);
  out_ptr = input.data_ptr();
  out_32b_ok = tensorCanUse32BitIndexMath(input);
}


// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                            IMPLEMENTATION
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

template <typename scalar_t, typename offset_t>
class MultiResImpl {
  typedef MultiResImpl Self;
  typedef void (Self::*ResizeFn)(offset_t x, offset_t y, offset_t z, offset_t n) const;
public:

  // ~~~ CONSTRUCTOR ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  ResizeImpl(const ResizeAllocator & info):

#define COPY_FROM_INFO(name) name(info.name)
#define COPY_FROM_INFO3(name) \
    name##0(info.name##0), name##1(info.name##1), name##2(info.name##2) 

    COPY_FROM_INFO(dim),
    COPY_FROM_INFO3(bound),
    COPY_FROM_INFO3(scale),
    COPY_FROM_INFO(do_adjoint),
    COPY_FROM_INFO(N),
    COPY_FROM_INFO(C),

#define INIT_ALLOC_INFO_5D(NAME) \
    NAME##_X(static_cast<offset_t>(info.NAME##_X)),   \
    NAME##_Y(static_cast<offset_t>(info.NAME##_Y)),   \
    NAME##_Z(static_cast<offset_t>(info.NAME##_Z)),   \
    NAME##_sN(static_cast<offset_t>(info.NAME##_sN)), \
    NAME##_sC(static_cast<offset_t>(info.NAME##_sC)), \
    NAME##_sX(static_cast<offset_t>(info.NAME##_sX)), \
    NAME##_sY(static_cast<offset_t>(info.NAME##_sY)), \
    NAME##_sZ(static_cast<offset_t>(info.NAME##_sZ)), \
    NAME##_ptr(static_cast<scalar_t*>(info.NAME##_ptr))

    INIT_ALLOC_INFO_5D(inp),
    INIT_ALLOC_INFO_5D(out)
  {
#ifndef __CUDACC__
      set_resize();
#endif
  }

#ifndef __CUDACC__
  NI_HOST NI_INLINE void set_resize() 
  {
#   define ADJ 4
    uint8_t mode = dim + ADJ * do_adjoint;
    switch (mode) {
      case 1:
        resize_ = &Self::resize1d; break;
      case 2:
        resize_ = &Self::resize2d; break;
      case 3:
        resize_ = &Self::resize3d; break;
      case 1+ADJ:
        resize_ = &Self::restrict1d; break; 
      case 2+ADJ:
        resize_ = &Self::restrict2d; break;
      case 3+ADJ:
        resize_ = &Self::restrict3d; break;
      default:
        resize_ = &Self::resize3d; break;
    }
  }
#endif

  // ~~~ FUNCTORS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#ifdef __CUDACC__
  // Loop over voxels that belong to one CUDA block
  // This function is called by the CUDA kernel
  NI_DEVICE void loop(int threadIdx, int blockIdx, 
                      int blockDim, int gridDim) const;
#else
  // Loop over all voxels
  void loop() const;
#endif

  NI_HOST NI_DEVICE int64_t voxcount() const { 
    return N * out_X * out_Y * out_Z;
  }

private:

  // ~~~ COMPONENTS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  NI_DEVICE NI_INLINE void resize(
    offset_t w, offset_t h, offset_t d, offset_t n) const;

#define DECLARE_RESIZE(name) \
  NI_DEVICE void name( \
    offset_t w, offset_t h, offset_t d, offset_t n) const;

  DECLARE_RESIZE(resize1d)
  DECLARE_RESIZE(resize2d)
  DECLARE_RESIZE(resize3d)

  DECLARE_RESIZE(restrict1d)
  DECLARE_RESIZE(restrict2d)
  DECLARE_RESIZE(restrict3d)

  // ~~~ OPTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  int               dim;            // dimensionality (1 or 2 or 3)
  BoundType         bound0;         // boundary condition  // x|W
  BoundType         bound1;         // boundary condition  // y|H
  BoundType         bound2;         // boundary condition  // z|D
  double            scale0;         // scale               // x|W
  double            scale1;         // scale               // y|H
  double            scale2;         // scale               // z|D
  bool              do_adjoint;     // push instead of pull
#ifndef __CUDACC__
  ResizeFn          resize_;        // Pointer to resize function
#endif

  // ~~~ NAVIGATORS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#define DECLARE_STRIDE_INFO_5D(NAME) \
  offset_t NAME##_X;                \
  offset_t NAME##_Y;                \
  offset_t NAME##_Z;                \
  offset_t NAME##_sN;               \
  offset_t NAME##_sC;               \
  offset_t NAME##_sX;               \
  offset_t NAME##_sY;               \
  offset_t NAME##_sZ;               \
  scalar_t * NAME##_ptr;

  offset_t N;
  offset_t C;
  DECLARE_STRIDE_INFO_5D(inp)
  DECLARE_STRIDE_INFO_5D(out)
};

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                             LOOP
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize(
    offset_t x, offset_t y, offset_t z, offset_t n) const {
#ifdef __CUDACC__
    // dispatch
#   define ADJ 4
    uint8_t mode = dim + ADJ * do_adjoint;
    switch (mode) {
      case 1:
        return resize1d(x, y, z, n);
      case 2:
        return resize2d(x, y, z, n);
      case 3:
        return resize3d(x, y, z, n);
      case 1+ADJ:
        return restrict1d(x, y, z, n); 
      case 2+ADJ:
        return restrict2d(x, y, z, n);
      case 3+ADJ:
        return restrict3d(x, y, z, n);
      default:
        return resize3d(x, y, z, n);
    }
#else
    CALL_MEMBER_FN(*this, resize_)(x, y, z, n);
#endif
}

#ifdef __CUDACC__

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::loop(
  int threadIdx, int blockIdx, int blockDim, int gridDim) const {

  offset_t index    = blockIdx * blockDim + threadIdx;
  offset_t out_YZ   =   out_Z * out_Y;
  offset_t out_XYZ  =  out_YZ * out_X;
  offset_t out_NXYZ = out_XYZ * N;
  offset_t n, w, h, d;
  for (offset_t i=index; index < out_NXYZ; index += blockDim*gridDim, i=index)
  {
      // Convert index: linear to sub
      n  = (i/out_XYZ);
      w  = (i/out_YZ) % out_X;
      h  = (i/out_Z)  % out_Y;
      d  = i % out_Z;

      resize(w, h, d, n);
  }
}

#else

// This bit loops over all output voxels. We therefore need to
// convert linear indices to multivariate indices. The way I do it
// might not be optimal.
// Note that I parallelize across all voxels (wheareas ATen's grid 
// sampler is only parallelized across batches).
//
// TODO: check that the default grain size is optimal. We do quite a lot
//       of compute per voxel, so a smaller value might be better suited.
template <typename scalar_t, typename offset_t> NI_HOST
void ResizeImpl<scalar_t,offset_t>::loop() const
{
  if (!has_atomic_add<scalar_t>::value && (do_adjoint))
  {
    // I do not have access to atomic operations so I cannot
    // parallelize across voxels.
    at::parallel_for(0, N, 0, [&](offset_t start, offset_t end) {
      for (offset_t n = start; n < end; ++n) {
        if (dim == 1) {
          for (offset_t w=0; w<out_X; ++w)
            resize(w, 0, 0, n);
        } else if (dim == 2) {
          for (offset_t h=0; h<out_Y; ++h)
          for (offset_t w=0; w<out_X; ++w)
            resize(w, h, 0, n);
        } else {
          for (offset_t d=0; d<out_Z; ++d)
          for (offset_t h=0; h<out_Y; ++h)
          for (offset_t w=0; w<out_X; ++w)
            resize(w, h, d, n);
        }
      }
    });
    return;
  }

  // Parallelize across voxels   
  offset_t out_YZ   =   out_Z * out_Y;
  offset_t out_XYZ  =  out_YZ * out_X;
  offset_t out_NXYZ = out_XYZ * N;
  at::parallel_for(0, out_NXYZ, GRAIN_SIZE,
                   [&](offset_t start, offset_t end) {
    offset_t n, w, h, d;
    for (offset_t i = start; i < end; ++i) {
      // Convert index: linear to sub
      n  = (i/out_XYZ);
      w  = (i/out_YZ) % out_X;
      h  = (i/out_Z)  % out_Y;
      d  = i % out_Z;

      resize(w, h, d, n);
    }
  }); 
}

#endif


// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     QUADRATIC PROLONGATION 3D
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#undef  GET_INDEX
#define GET_INDEX \
  scalar_t x = (w + 0.5) * scale0 - 0.5; \
  scalar_t y = (h + 0.5) * scale1 - 0.5; \
  scalar_t z = (d + 0.5) * scale2 - 0.5; \
  offset_t ix1 = static_cast<offset_t>(std::floor(x+0.5)); \
  offset_t iy1 = static_cast<offset_t>(std::floor(y+0.5)); \
  offset_t iz1 = static_cast<offset_t>(std::floor(z+0.5)); \
  scalar_t dx1 = interpolation::weight(Q, x - ix1); \
  scalar_t dy1 = interpolation::weight(Q, y - iy1); \
  scalar_t dz1 = interpolation::weight(Q, z - iz1); \
  scalar_t dx0 = interpolation::fastweight(Q, x - (ix1 - 1)); \
  scalar_t dy0 = interpolation::fastweight(Q, y - (iy1 - 1)); \
  scalar_t dz0 = interpolation::fastweight(Q, z - (iz1 - 1)); \
  scalar_t dx2 = interpolation::fastweight(Q, (ix1 + 1) - x); \
  scalar_t dy2 = interpolation::fastweight(Q, (iy1 + 1) - y); \
  scalar_t dz2 = interpolation::fastweight(Q, (iz1 + 1) - z); \
  scalar_t w000 = dx0 * dy0; \
  scalar_t w001 = w000 * dz1; \
  scalar_t w002 = w000 * dz2; \
           w000 *= dz0; \
  scalar_t w010 = dx0 * dy1; \
  scalar_t w011 = w010 * dz1; \
  scalar_t w012 = w010 * dz2; \
           w010 *= dz0; \
  scalar_t w020 = dx0 * dy2; \
  scalar_t w021 = w020 * dz1; \
  scalar_t w022 = w020 * dz2; \
           w020 *= dz0; \
  scalar_t w100 = dx1 * dy0; \
  scalar_t w101 = w100 * dz1; \
  scalar_t w102 = w100 * dz2; \
           w100 *= dz0; \
  scalar_t w110 = dx1 * dy1; \
  scalar_t w111 = w110 * dz1; \
  scalar_t w112 = w110 * dz2; \
           w110 *= dz0; \
  scalar_t w120 = dx1 * dy2; \
  scalar_t w121 = w120 * dz1; \
  scalar_t w122 = w120 * dz2; \
           w120 *= dz0; \
  scalar_t w200 = dx2 * dy0; \
  scalar_t w201 = w200 * dz1; \
  scalar_t w202 = w200 * dz2; \
           w200 *= dz0; \
  scalar_t w210 = dx2 * dy1; \
  scalar_t w211 = w210 * dz1; \
  scalar_t w212 = w210 * dz2; \
           w210 *= dz0; \
  scalar_t w220 = dx2 * dy2; \
  scalar_t w221 = w220 * dz1; \
  scalar_t w222 = w220 * dz2; \
           w220 *= dz0; \
  int8_t  sx0 = bound::sign(bound0, ix1-1, inp_X); \
  int8_t  sy0 = bound::sign(bound1, iy1-1, inp_Y); \
  int8_t  sz0 = bound::sign(bound2, iz1-1, inp_Z); \
  int8_t  sx2 = bound::sign(bound0, ix1+1, inp_X); \
  int8_t  sy2 = bound::sign(bound1, iy1+1, inp_Y); \
  int8_t  sz2 = bound::sign(bound2, iz1+1, inp_Z); \
  int8_t  sx1 = bound::sign(bound0, ix1,   inp_X); \
  int8_t  sy1 = bound::sign(bound1, iy1,   inp_Y); \
  int8_t  sz1 = bound::sign(bound2, iz1,   inp_Z); \
  int8_t s000 = sx0 * sy0; \
  int8_t s001 = s000 * sz1; \
  int8_t s002 = s000 * sz2; \
         s000 *= sz0; \
  int8_t s010 = sx0 * sy1; \
  int8_t s011 = s010 * sz1; \
  int8_t s012 = s010 * sz2; \
         s010 *= sz0; \
  int8_t s020 = sx0 * sy2; \
  int8_t s021 = s020 * sz1; \
  int8_t s022 = s020 * sz2; \
         s020 *= sz0; \
  int8_t s100 = sx1 * sy0; \
  int8_t s101 = s100 * sz1; \
  int8_t s102 = s100 * sz2; \
         s100 *= sz0; \
  int8_t s110 = sx1 * sy1; \
  int8_t s111 = s110 * sz1; \
  int8_t s112 = s110 * sz2; \
         s110 *= sz0; \
  int8_t s120 = sx1 * sy2; \
  int8_t s121 = s120 * sz1; \
  int8_t s122 = s120 * sz2; \
         s120 *= sz0; \
  int8_t s200 = sx2 * sy0; \
  int8_t s201 = s200 * sz1; \
  int8_t s202 = s200 * sz2; \
         s200 *= sz0; \
  int8_t s210 = sx2 * sy1; \
  int8_t s211 = s210 * sz1; \
  int8_t s212 = s210 * sz2; \
         s210 *= sz0; \
  int8_t s220 = sx2 * sy2; \
  int8_t s221 = s220 * sz1; \
  int8_t s222 = s220 * sz2; \
         s220 *= sz0; \
  offset_t ix0, iy0, iz0, ix2, iy2, iz2; \
  ix0 = bound::index(bound0, ix1-1, inp_X) * inp_sX; \
  iy0 = bound::index(bound1, iy1-1, inp_Y) * inp_sY; \
  iz0 = bound::index(bound2, iz1-1, inp_Z) * inp_sZ; \
  ix2 = bound::index(bound0, ix1+1, inp_X) * inp_sX; \
  iy2 = bound::index(bound1, iy1+1, inp_Y) * inp_sY; \
  iz2 = bound::index(bound2, iz1+1, inp_Z) * inp_sZ; \
  ix1 = bound::index(bound0, ix1,   inp_X) * inp_sX; \
  iy1 = bound::index(bound1, iy1,   inp_Y) * inp_sY; \
  iz1 = bound::index(bound2, iz1,   inp_Z) * inp_sZ; \
  offset_t o000 = ix0 + iy0 + iz0; \
  offset_t o001 = ix0 + iy0 + iz1; \
  offset_t o002 = ix0 + iy0 + iz2; \
  offset_t o010 = ix0 + iy1 + iz0; \
  offset_t o011 = ix0 + iy1 + iz1; \
  offset_t o012 = ix0 + iy1 + iz2; \
  offset_t o020 = ix0 + iy2 + iz0; \
  offset_t o021 = ix0 + iy2 + iz1; \
  offset_t o022 = ix0 + iy2 + iz2; \
  offset_t o100 = ix1 + iy0 + iz0; \
  offset_t o101 = ix1 + iy0 + iz1; \
  offset_t o102 = ix1 + iy0 + iz2; \
  offset_t o110 = ix1 + iy1 + iz0; \
  offset_t o111 = ix1 + iy1 + iz1; \
  offset_t o112 = ix1 + iy1 + iz2; \
  offset_t o120 = ix1 + iy2 + iz0; \
  offset_t o121 = ix1 + iy2 + iz1; \
  offset_t o122 = ix1 + iy2 + iz2; \
  offset_t o200 = ix2 + iy0 + iz0; \
  offset_t o201 = ix2 + iy0 + iz1; \
  offset_t o202 = ix2 + iy0 + iz2; \
  offset_t o210 = ix2 + iy1 + iz0; \
  offset_t o211 = ix2 + iy1 + iz1; \
  offset_t o212 = ix2 + iy1 + iz2; \
  offset_t o220 = ix2 + iy2 + iz0; \
  offset_t o221 = ix2 + iy2 + iz1; \
  offset_t o222 = ix2 + iy2 + iz2; \
  scalar_t *out_ptr_NCXYZ = out_ptr                   \
                          + n * out_sN + w * out_sX   \
                          + h * out_sY + d * out_sZ;  \
  scalar_t *inp_ptr_NC = inp_ptr + n * inp_sN;

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize3d(
  offset_t w, offset_t h, offset_t d, offset_t n) const
{
  GET_INDEX

  // NB: The brackets in the sum matter a lot!
  // If they're removed, inacuracies creep in and the result of FMG is crap.
  // I've gathered terms by distance to the center voxel.

  for (offset_t c = 0; c < C; ++c, out_ptr_NCXYZ += out_sC, 
                                   inp_ptr_NC    += inp_sC) {
    *out_ptr_NCXYZ = bound::get(inp_ptr_NC, o111, s111) * w111
                   +(bound::get(inp_ptr_NC, o011, s011) * w011
                   + bound::get(inp_ptr_NC, o101, s101) * w101
                   + bound::get(inp_ptr_NC, o110, s110) * w110
                   + bound::get(inp_ptr_NC, o112, s112) * w112
                   + bound::get(inp_ptr_NC, o121, s121) * w121
                   + bound::get(inp_ptr_NC, o211, s211) * w211)
                   +(bound::get(inp_ptr_NC, o001, s001) * w001
                   + bound::get(inp_ptr_NC, o010, s010) * w010
                   + bound::get(inp_ptr_NC, o100, s100) * w100
                   + bound::get(inp_ptr_NC, o012, s012) * w012
                   + bound::get(inp_ptr_NC, o021, s021) * w021
                   + bound::get(inp_ptr_NC, o201, s201) * w201
                   + bound::get(inp_ptr_NC, o210, s210) * w210
                   + bound::get(inp_ptr_NC, o212, s212) * w212
                   + bound::get(inp_ptr_NC, o221, s221) * w221
                   + bound::get(inp_ptr_NC, o120, s120) * w120
                   + bound::get(inp_ptr_NC, o122, s122) * w122
                   + bound::get(inp_ptr_NC, o102, s102) * w102)
                   +(bound::get(inp_ptr_NC, o000, s000) * w000
                   + bound::get(inp_ptr_NC, o002, s002) * w002
                   + bound::get(inp_ptr_NC, o020, s020) * w020
                   + bound::get(inp_ptr_NC, o200, s200) * w200
                   + bound::get(inp_ptr_NC, o022, s022) * w022
                   + bound::get(inp_ptr_NC, o202, s202) * w202
                   + bound::get(inp_ptr_NC, o220, s220) * w220
                   + bound::get(inp_ptr_NC, o222, s222) * w222);
  }
}


template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize2d(offset_t w, offset_t h, offset_t d, offset_t n) const {}
template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize1d(offset_t w, offset_t h, offset_t d, offset_t n) const {}

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                     LINEAR RESTRICTION 3D
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#undef  GET_INDEX
#define GET_INDEX \
  scalar_t x = (w + 0.5) / scale0 - 0.5; \
  scalar_t y = (h + 0.5) / scale1 - 0.5; \
  scalar_t z = (d + 0.5) / scale2 - 0.5; \
  offset_t ix1 = static_cast<offset_t>(std::floor(x)); \
  offset_t iy1 = static_cast<offset_t>(std::floor(y)); \
  offset_t iz1 = static_cast<offset_t>(std::floor(z)); \
  scalar_t dx1 = x - ix0; \
  scalar_t dy1 = y - iy0; \
  scalar_t dz1 = z - iz0; \
  scalar_t dx0 = 1. - dx1; \
  scalar_t dy0 = 1. - dy1; \
  scalar_t dz0 = 1. - dz1; \
  scalar_t w000 = dx0 * dy0; \
  scalar_t w001 = w000 * dz1; \
           w000 *= dz0; \
  scalar_t w010 = dx0 * dy1; \
  scalar_t w011 = w010 * dz1; \
           w010 *= dz0; \
  scalar_t w100 = dx1 * dy0; \
  scalar_t w101 = w100 * dz1; \
           w100 *= dz0; \
  scalar_t w110 = dx1 * dy1; \
  scalar_t w111 = w110 * dz1; \
           w110 *= dz0; \
  int8_t  sx3 = bound::sign(bound0, ix1+2, inp_X); \
  int8_t  sy3 = bound::sign(bound1, iy1+2, inp_Y); \
  int8_t  sz3 = bound::sign(bound2, iz1+2, inp_Z); \
  int8_t  sx2 = bound::sign(bound0, ix1+1, inp_X); \
  int8_t  sy2 = bound::sign(bound1, iy1+1, inp_Y); \
  int8_t  sz2 = bound::sign(bound2, iz1+1, inp_Z); \
  int8_t  sx0 = bound::sign(bound0, ix1-1, inp_X); \
  int8_t  sy0 = bound::sign(bound1, iy1-1, inp_Y); \
  int8_t  sz0 = bound::sign(bound2, iz1-1, inp_Z); \
  int8_t  sx1 = bound::sign(bound0, ix1,   inp_X); \
  int8_t  sy1 = bound::sign(bound1, iy1,   inp_Y); \
  int8_t  sz1 = bound::sign(bound2, iz1,   inp_Z); \
  int8_t  s000 = sx0 * sy0; \
  int8_t  s001 = s000 * sz1; \
          s000 *= sz0; \
  int8_t  s010 = sx0 * sy1; \
  int8_t  s011 = s010 * sz1; \
          s010 *= sz0; \
  int8_t  s100 = sx1 * sy0; \
  int8_t  s101 = s100 * sz1; \
          s100 *= sz0; \
  int8_t  s110 = sx1 * sy1; \
  int8_t  s111 = s110 * sz1; \
          s110 *= sz0; \
  offset_t ix1, iy1, iz1; \
  ix3 = bound::index(bound0, ix1+2, inp_X) * inp_sX; \
  iy3 = bound::index(bound1, iy1+2, inp_Y) * inp_sY; \
  iz3 = bound::index(bound2, iz1+2, inp_Z) * inp_sZ; \
  ix2 = bound::index(bound0, ix1+1, inp_X) * inp_sX; \
  iy2 = bound::index(bound1, iy1+1, inp_Y) * inp_sY; \
  iz2 = bound::index(bound2, iz1+1, inp_Z) * inp_sZ; \
  ix0 = bound::index(bound0, ix1-1, inp_X) * inp_sX; \
  iy0 = bound::index(bound1, iy1-1, inp_Y) * inp_sY; \
  iz0 = bound::index(bound2, iz1-1, inp_Z) * inp_sZ; \
  ix1 = bound::index(bound0, ix1,   inp_X) * inp_sX; \
  iy1 = bound::index(bound1, iy1,   inp_Y) * inp_sY; \
  iz1 = bound::index(bound2, iz1,   inp_Z) * inp_sZ; \
  offset_t o000, o100, o010, o001, o110, o011, o101, o111; \
  o000 = ix0 + iy0; \
  o001 = o000 + iz1; \
  o000 += iz0; \
  o010 = ix0 + iy1; \
  o011 = o010 + iz1; \
  o010 += iz0; \
  o100 = ix1 + iy0; \
  o101 = o100 + iz1; \
  o100 += iz0; \
  o110 = ix1 + iy1; \
  o111 = o110 + iz1; \
  o110 += iz0; \
  scalar_t *out_ptr_NCXYZ = out_ptr                   \
                          + n * out_sN + w * out_sX   \
                          + h * out_sY + d * out_sZ;  \
  scalar_t *inp_ptr_NC = inp_ptr + n * inp_sN;

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict3d(
  offset_t w, offset_t h, offset_t d, offset_t n) const
{
  GET_INDEX

  for (offset_t c = 0; c < C; ++c, out_ptr_NCXYZ += out_sC, 
                                   inp_ptr_NC    += inp_sC) {
    *out_ptr_NCXYZ =(bound::get(inp_ptr_NC, o111, s111) * w111
                   + bound::get(inp_ptr_NC, o112, s112) * w112
                   + bound::get(inp_ptr_NC, o121, s121) * w121
                   + bound::get(inp_ptr_NC, o122, s122) * w122
                   + bound::get(inp_ptr_NC, o211, s211) * w211
                   + bound::get(inp_ptr_NC, o212, s212) * w212
                   + bound::get(inp_ptr_NC, o221, s221) * w221
                   + bound::get(inp_ptr_NC, o222, s222) * w222)
                     // sides
                  +((bound::get(inp_ptr_NC, o011, s011) * w011
                   + bound::get(inp_ptr_NC, o012, s012) * w012
                   + bound::get(inp_ptr_NC, o021, s021) * w021
                   + bound::get(inp_ptr_NC, o022, s022) * w022)
                   +(bound::get(inp_ptr_NC, o311, s311) * w311
                   + bound::get(inp_ptr_NC, o312, s312) * w312
                   + bound::get(inp_ptr_NC, o321, s321) * w321
                   + bound::get(inp_ptr_NC, o322, s322) * w322)
                   +(bound::get(inp_ptr_NC, o101, s101) * w101
                   + bound::get(inp_ptr_NC, o102, s102) * w102
                   + bound::get(inp_ptr_NC, o201, s201) * w201
                   + bound::get(inp_ptr_NC, o202, s202) * w202)
                   +(bound::get(inp_ptr_NC, o131, s131) * w131
                   + bound::get(inp_ptr_NC, o132, s132) * w132
                   + bound::get(inp_ptr_NC, o231, s231) * w231
                   + bound::get(inp_ptr_NC, o232, s232) * w232)
                   +(bound::get(inp_ptr_NC, o110, s110) * w110
                   + bound::get(inp_ptr_NC, o120, s120) * w120
                   + bound::get(inp_ptr_NC, o210, s210) * w210
                   + bound::get(inp_ptr_NC, o220, s220) * w220)
                   +(bound::get(inp_ptr_NC, o113, s113) * w113
                   + bound::get(inp_ptr_NC, o123, s123) * w123
                   + bound::get(inp_ptr_NC, o213, s213) * w213
                   + bound::get(inp_ptr_NC, o223, s223) * w223))
                     // 2d corners
                   +(bound::get(inp_ptr_NC, o001, s001) * w001
                   + bound::get(inp_ptr_NC, o002, s002) * w002
                   + bound::get(inp_ptr_NC, o331, s331) * w331
                   + bound::get(inp_ptr_NC, o332, s332) * w332
                   + bound::get(inp_ptr_NC, o010, s010) * w010
                   + bound::get(inp_ptr_NC, o020, s020) * w020
                   + bound::get(inp_ptr_NC, o313, s313) * w313
                   + bound::get(inp_ptr_NC, o323, s323) * w323
                   + bound::get(inp_ptr_NC, o100, s100) * w100
                   + bound::get(inp_ptr_NC, o200, s200) * w200
                   + bound::get(inp_ptr_NC, o133, s133) * w133
                   + bound::get(inp_ptr_NC, o233, s233) * w233)
                     // 3d corners
                   +(bound::get(inp_ptr_NC, o000, s000) * w000
                   + bound::get(inp_ptr_NC, o003, s003) * w003
                   + bound::get(inp_ptr_NC, o030, s030) * w030
                   + bound::get(inp_ptr_NC, o033, s033) * w033
                   + bound::get(inp_ptr_NC, o300, s300) * w300
                   + bound::get(inp_ptr_NC, o303, s303) * w303
                   + bound::get(inp_ptr_NC, o330, s330) * w330
                   + bound::get(inp_ptr_NC, o333, s333) * w333);
  }
}

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict2d(offset_t w, offset_t h, offset_t d, offset_t n) const {}
template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict1d(offset_t w, offset_t h, offset_t d, offset_t n) const {}

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                  NEAREST NEIGHBOR INTERPOLATION 3D
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#undef  GET_INDEX
#define GET_INDEX \
  offset_t ix = static_cast<offset_t>(std::round(scale0 * w + shift0)); \
  offset_t iy = static_cast<offset_t>(std::round(scale1 * h + shift1)); \
  offset_t iz = static_cast<offset_t>(std::round(scale2 * d + shift1)); \
  int8_t   sx = bound::sign(bound0, ix, inp_X);                         \
  int8_t   sy = bound::sign(bound1, iy, inp_Y);                         \
  int8_t   sz = bound::sign(bound2, iz, inp_Z);                         \
           ix = bound::index(bound0, ix,inp_X);                         \
           iy = bound::index(bound1, iy,inp_Y);                         \
           iz = bound::index(bound2, iz,inp_Z);                         \
  int8_t    s = sz * sy * sx;                                           \
  offset_t  o = iz*inp_sZ + iy*inp_sY + ix*inp_sX;                      \
  scalar_t *out_ptr_NCXYZ = out_ptr + n * out_sN + w * out_sX           \
                                    + h * out_sY + d * out_sZ;          \
  scalar_t *inp_ptr_NC = inp_ptr + n * inp_sN;


template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize3d_nearest(
  offset_t w, offset_t h, offset_t d, offset_t n) const
{
  GET_INDEX
  for (offset_t c = 0; c < C; ++c, out_ptr_NCXYZ += out_sC, 
                                   inp_ptr_NC    += inp_sC)
    bound::add(inp_ptr_NC, o, *out_ptr_NCXYZ, s);
}

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict3d_nearest(
  offset_t w, offset_t h, offset_t d, offset_t n) const
{
  GET_INDEX
  for (offset_t c = 0; c < C; ++c, out_ptr_NCXYZ += out_sC, 
                                   inp_ptr_NC     += inp_sC)
    *out_ptr_NCXYZ = bound::get(inp_ptr_NC, o, s);
}

template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize2d_nearest(offset_t w, offset_t h, offset_t d, offset_t n) const {}
template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict2d_nearest(offset_t w, offset_t h, offset_t d, offset_t n) const {}
template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::resize1d_nearest(offset_t w, offset_t h, offset_t d, offset_t n) const {}
template <typename scalar_t, typename offset_t> NI_DEVICE
void ResizeImpl<scalar_t,offset_t>::restrict1d_nearest(offset_t w, offset_t h, offset_t d, offset_t n) const {}

// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                  CUDA KERNEL (MUST BE OUT OF CLASS)
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#ifdef __CUDACC__
// CUDA Kernel
template <typename scalar_t, typename offset_t>
C10_LAUNCH_BOUNDS_1(1024)
__global__ void resize_kernel(ResizeImpl<scalar_t,offset_t> * f) {
  f->loop(threadIdx.x, blockIdx.x, blockDim.x, gridDim.x);
}
#endif


// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
//                    FUNCTIONAL FORM WITH DISPATCH
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NI_HOST NI_INLINE void
check_same_nonspatial(Tensor input, Tensor output)
{
  bool same_nonspatial = (input.dim()   == output.dim())    &&
                         (input.size(0) == output.size(0))  &&
                         (input.size(1) == output.size(1));
  if (!same_nonspatial) {
    std::string const msg = "Source and output should have the same "
                            "batch and channel shapes but found dims "
                          + std::to_string(input.dim()) + " vs " 
                          + std::to_string(output.dim()) + " and shapes ["
                          + std::to_string(input.size(0)) + " " 
                          + std::to_string(input.size(1)) + "] vs ["
                          + std::to_string(output.size(0)) + " " 
                          + std::to_string(output.size(1)) + "].";
    throw std::invalid_argument(msg);
  }
}

NI_HOST NI_INLINE std::tuple<Tensor, Tensor>
prepare_tensors(Tensor input, Tensor output, ArrayRef<double> factor, bool do_adjoint)
{
  bool input_defined = (input.defined() && input.numel() > 0);
  bool output_defined = (output.defined() && output.numel() > 0);

  if (input_defined && output_defined) {
    check_same_nonspatial(input, output);
    if (do_adjoint)
      input.zero_();
    return std::tuple<Tensor, Tensor>(input, output);
  }

  double fx = factor.size() > 0 ? factor[0] : 1.;
  double fy = factor.size() > 1 ? factor[1] : fx;
  double fz = factor.size() > 2 ? factor[2] : fy;
  if (do_adjoint) {
    fx = 1./fx;
    fy = 1./fy;
    fz = 1./fz;
  }

  if (input_defined) { // PULL

    int64_t dim = input.dim() - 2;
    int64_t N = input.size(0);
    int64_t C = input.size(1);
    int64_t X = input.size(2);
    int64_t Y = dim > 1 ? input.size(3) : 1L;
    int64_t Z = dim > 2 ? input.size(4) : 1L;

    int64_t Xt =           static_cast<int64_t>(std::floor(static_cast<double>(X) * fx));
    int64_t Yt = dim > 1 ? static_cast<int64_t>(std::floor(static_cast<double>(Y) * fy)) : 1L;
    int64_t Zt = dim > 2 ? static_cast<int64_t>(std::floor(static_cast<double>(Z) * fz)) : 1L;

    output = at::zeros({N, C, Xt, Yt, Zt}, input.options());

  } else { // PUSH

    int64_t dim = output.dim() - 2;
    int64_t N = output.size(0);
    int64_t C = output.size(1);
    int64_t X = output.size(2);
    int64_t Y = dim > 1 ? output.size(3) : 1L;
    int64_t Z = dim > 2 ? output.size(4) : 1L;

    int64_t Xs =           static_cast<int64_t>(std::ceil(static_cast<double>(X) * fx));
    int64_t Ys = dim > 1 ? static_cast<int64_t>(std::ceil(static_cast<double>(Y) * fy)) : 1L;
    int64_t Zs = dim > 2 ? static_cast<int64_t>(std::ceil(static_cast<double>(Z) * fz)) : 1L;

    input = at::zeros({N, C, Xs, Ys, Zs}, output.options());

  }

  return std::tuple<Tensor, Tensor>(input, output);
}

NI_HOST NI_INLINE  std::pair<double, double>
prepare_affine(double factor, double Ds, double Dt, GridAlignType mode)
{
  double shift = 0., scale = 1./factor;
  switch (mode) {
    case GridAlignType::Edge:
      shift = (Ds / Dt - 1);
      scale = Ds / Dt;
      break;
    case GridAlignType::Center:
      shift = 0.;
      scale = (Ds - 1) / (Dt - 1);
      break;
    case GridAlignType::Last:
      shift = factor * (Ds - 1) / (Dt - 1);
      break;
    default:
      break;
  }
  return std::make_pair(shift, scale);
}

NI_HOST NI_INLINE std::pair<std::vector<double>,  std::vector<double> >
prepare_affines(Tensor input, Tensor output, ArrayRef<double> factor, GridAlignVectorRef mode, bool do_adjoint)
{

  double fx = factor.size() > 0 ? factor[0] : 1.;
  double fy = factor.size() > 1 ? factor[1] : fx;
  double fz = factor.size() > 2 ? factor[2] : fy;
  if (do_adjoint) {
    fx = 1./fx;
    fy = 1./fy;
    fz = 1./fz;
  }

  GridAlignType mx = mode.size() > 0 ? mode[0] : GridAlignType::Center;
  GridAlignType my = mode.size() > 1 ? mode[1] : mx;
  GridAlignType mz = mode.size() > 2 ? mode[2] : my;

  int64_t dim = output.dim() - 2;
  int64_t Xt  = output.size(2);
  int64_t Yt  = dim > 1 ? output.size(3) : 1L;
  int64_t Zt  = dim > 2 ? output.size(4) : 1L;
  int64_t Xs  = input.size(2);
  int64_t Ys  = dim > 1 ? input.size(3) : 1L;
  int64_t Zs  = dim > 2 ? input.size(4) : 1L;

  auto affinex = prepare_affine(fx, static_cast<double>(Xs), static_cast<double>(Xt), mx);
  auto affiney = prepare_affine(fy, static_cast<double>(Ys), static_cast<double>(Yt), my);
  auto affinez = prepare_affine(fz, static_cast<double>(Zs), static_cast<double>(Zt), mz);

  return make_pair(
      std::vector<double>({std::get<0>(affinex), std::get<0>(affiney), std::get<0>(affinez)}),
      std::vector<double>({std::get<1>(affinex), std::get<1>(affiney), std::get<1>(affinez)})
  );
}

} // namespace

#ifdef __CUDACC__

// ~~~ CUDA ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
NI_HOST
Tensor resize_impl(
  Tensor input, Tensor output, ArrayRef<double> factor,
  BoundVectorRef bound, InterpolationVectorRef interpolation, 
  GridAlignVectorRef mode, bool do_adjoint, bool normalize)
{
  auto tensors = prepare_tensors(input, output, factor, do_adjoint);
  input = std::get<0>(tensors);
  output = std::get<1>(tensors);
  auto affines = prepare_affines(input, output, factor, mode, do_adjoint);
  ArrayRef<double> shifts(std::get<0>(affines));
  ArrayRef<double> scales(std::get<1>(affines));

  ResizeAllocator info(input.dim()-2, bound, interpolation, shifts, scales, do_adjoint > 0);
  info.ioset(input, output);
  auto stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "resize_impl", [&] {
    if (info.canUse32BitIndexMath())
    {
      ResizeImpl<scalar_t, int32_t> algo(info);
      auto palgo = alloc_and_copy_to_device(algo, stream);
      resize_kernel<<<GET_BLOCKS(algo.voxcount()), CUDA_NUM_THREADS, 0, stream>>>(palgo);
      cudaFree(palgo);
    }
    else
    {
      ResizeImpl<scalar_t, int64_t> algo(info);
      auto palgo = alloc_and_copy_to_device(algo, stream);
      resize_kernel<<<GET_BLOCKS(algo.voxcount()), CUDA_NUM_THREADS, 0, stream>>>(palgo);
      cudaFree(palgo);
    }
  });

  Tensor out = do_adjoint ? input : output;
  if (normalize)
    switch (out.dim() - 2) {
      case 1:  out *= scales[0];
      case 2:  out *= scales[0] * scales[1];
      default: out *= scales[0] * scales[1] * scales[2];
    }
  return out;
}

#else

// ~~~ CPU ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NI_HOST
Tensor resize_impl(
  Tensor input, Tensor output, ArrayRef<double> factor,
  BoundVectorRef bound, InterpolationVectorRef interpolation,  
  GridAlignVectorRef mode, bool do_adjoint, bool normalize)
{
  auto tensors = prepare_tensors(input, output, factor, do_adjoint);
  input = std::get<0>(tensors);
  output = std::get<1>(tensors);
  auto affines = prepare_affines(input, output, factor, mode, do_adjoint);
  ArrayRef<double> shifts(std::get<0>(affines));
  ArrayRef<double> scales(std::get<1>(affines));

  ResizeAllocator info(input.dim()-2, bound, interpolation, shifts, scales, do_adjoint > 0);
  info.ioset(input, output);

  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "resize_impl", [&] {
    ResizeImpl<scalar_t, int64_t> algo(info);
    algo.loop();
  });

  Tensor out = do_adjoint ? input : output;
  if (normalize)
    switch (out.dim() - 2) {
      case 1:  out *= scales[0];
      case 2:  out *= scales[0] * scales[1];
      default: out *= scales[0] * scales[1] * scales[2];
    }
  return out;
}

#endif // __CUDACC__


} // namespace <device>

// ~~~ NOT IMPLEMENTED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

namespace notimplemented {

NI_HOST
Tensor resize_impl(
  Tensor input, Tensor output, ArrayRef<double> factor,
  BoundVectorRef bound, InterpolationVectorRef interpolation, 
  GridAlignVectorRef mode, bool do_adjoint, bool normalize)
{
  throw std::logic_error("Function not implemented for this device.");
}

} // namespace notimplemented

} // namespace ni
